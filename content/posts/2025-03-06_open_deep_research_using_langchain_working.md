+++
title = "Open Deep Research using LangChain *Working*"

[taxonomies]
tags = ["Research", "AI", "Agents", "Nix", "langchain"]
+++

I have Open Deep Research using LangChain working. This builds upon the work in
[Open Deep Research Using Langchain](@/posts/2025-02-26_open_deep_research_using_langchain.md).
Just on a lark, I ran `git pull` and there were changes. I resolved merge conflicts and
now it works!
<!--more-->

I will include a diff at the end so you can run this locally as well if you would like.
But first... here is the output of LangChain's Open Deep Seek running locally and
answering the query `Overview of the AI inference market with focus on Fireworks, Together.ai, Groq`.
This is the same query as in [the previous post](@/posts/2025-02-26_open_deep_research_using_langchain.md).

---

# Overview of the AI Inference Market with Focus on Fireworks, Together.ai, Groq

The AI inference market is experiencing rapid growth as businesses across industries seek real-time decision-making capabilities. Companies like Fireworks, Together.ai, and Groq are leading this charge by offering advanced solutions that enhance computational efficiency and scalability. These platforms enable faster processing of large datasets, supporting applications in autonomous vehicles, healthcare diagnostics, and financial services.

## Comparison of Fireworks, Together.ai, and Groq

| Feature/Company | Fireworks | Together.ai | Groq |
|-----------------|-----------|-------------|------|
| **Platform Focus** | Scalable AI inference for real-time applications | Collaborative AI tools integrated into workflows | High-performance computing using custom silicon technology |
| **Key Differentiators** | Ease of integration, low latency, supports various ML frameworks | Seamless workflow integration, accessible to non-experts, competitive pricing | Custom-built processor architecture (GroqFlow) for parallel processing |
| **Market Positioning** | Reliable choice for organizations leveraging AI technologies | Strong competitor in the growing AI inference market through productivity enhancements | Valuable for real-time decision-making and large-scale data analysis |

This report provides a detailed overview of these platforms, highlighting their unique features and market impact. As the demand for efficient AI solutions continues to rise, understanding the capabilities of Fireworks, Together.ai, and Groq is crucial for businesses looking to stay competitive.

## Overview of AI Inference Market

The AI inference market is rapidly growing, driven by increasing demand for real-time decision-making across various industries. Companies like Fireworks, Together.ai, and Groq are at the forefront, offering solutions that enhance computational efficiency and scalability. These advancements enable faster processing of large datasets, supporting applications in autonomous vehicles, healthcare diagnostics, and financial services [1].

### Sources
[1] McKinsey & Company: https://www.mckinsey.com/business-functions/mckinsey-digital/our-insights/artificial-intelligence-the-next-frontier-for-growth-in-retail

## Fireworks Overview

Fireworks is a platform designed to simplify the deployment of AI models at scale. It offers a user-friendly interface that allows developers and businesses to deploy, manage, and monitor AI applications efficiently [1]. One of its key features is the ability to handle large-scale inference tasks with ease, making it suitable for high-demand environments [2]. Fireworks also provides robust security measures and compliance support, ensuring data protection and regulatory adherence [3]. In terms of market position, Fireworks competes with other AI deployment platforms by offering a seamless integration process and cost-effective solutions, positioning itself as a reliable choice for organizations looking to leverage AI technologies.

### Sources
[1] Fireworks Documentation: https://docs.fireworks.ai/
[2] Fireworks Blog Post on Scalability: https://blog.fireworks.ai/scalable-ai-inference-made-easy/
[3] Fireworks Security Features Overview: https://fireworks.ai/security/

## Together.ai Overview

Together.ai is a platform designed to simplify access to advanced AI models for developers and businesses. It offers a suite of tools that enable users to deploy, manage, and scale AI applications efficiently [1]. One of its key differentiators is the ability to integrate seamlessly with existing workflows, making it accessible for non-experts in machine learning [2]. Additionally, Together.ai focuses on providing high-performance computing resources at competitive prices, which positions it as a strong competitor in the rapidly growing AI inference market [3].

### Sources
[1] Together.ai: https://together.xyz/
[2] Together.ai Documentation: https://docs.together.xyz/
[3] Market Analysis Report on AI Inference Platforms: https://www.marketsandmarkets.com/Market-Reports/artificial-intelligence-inference-market-106745897.html

## Groq Overview

Groq specializes in high-performance computing for artificial intelligence, offering advanced AI inference solutions designed to accelerate machine learning workloads. Their unique approach centers on the use of a custom-built processor architecture called the GroqFlow, which is optimized for parallel processing and can handle complex computations efficiently [1]. This technology enables significant speedups in AI model deployment and inference, making it particularly valuable for applications requiring real-time decision-making and large-scale data analysis.

### Sources
[1] Groq: https://groq.com/

# Revenue Estimate for Fireworks

Fireworks is poised to capture significant market share in the rapidly growing AI inference sector, driven by its user-friendly platform and focus on scalability. As organizations increasingly demand efficient deployment of AI models, Fireworks' seamless integration capabilities and cost-effective solutions make it a compelling choice. This section provides an estimated revenue forecast based on current market data and growth projections.

## Revenue Estimate for Fireworks

Fireworks has been gaining traction in the AI inference market due to its robust platform features and competitive pricing. Given the growing demand for scalable AI solutions, Fireworks is expected to see substantial revenue growth over the next few years. By leveraging strategic partnerships and expanding its customer base, Fireworks aims to achieve a significant market position.

To estimate Fireworks' revenue, we consider several factors including market size, growth rate, and competitive landscape. The global AI inference market is projected to reach $108 billion by 2030, growing at a compound annual growth rate (CAGR) of 24% [1]. Assuming Fireworks captures a conservative 1% share of this market in the next five years, its revenue could exceed $50 million annually.

## Conclusion

Fireworks is well-positioned to capitalize on the expanding AI inference market with its scalable and user-friendly platform. By focusing on strategic partnerships and enterprise adoption, Fireworks aims to achieve substantial revenue growth.

## Comparison Table of Key Players in AI Inference Market

| Company    | Key Features                                                                 | Revenue Estimate (2025) |
|------------|------------------------------------------------------------------------------|-------------------------|
| Fireworks  | Scalable AI inference platform, user-friendly interface                      | $50M+                   |
| Together.ai| Seamless integration with workflows, high-performance computing resources      | $30M+                   |
| Groq       | Custom silicon technology for parallel processing, exceptional throughput    | $70M+                   |

Next steps include further market analysis and strategic planning to enhance Fireworks' competitive edge in the AI inference sector.

[1] McKinsey & Company: https://www.mckinsey.com/business-functions/mckinsey-digital/our-insights/artificial-intelligence-the-next-frontier-for-growth-in-retail

# Revenue Estimate for Together.ai

Together.ai is emerging as a key player in the AI inference market, offering advanced tools that simplify access to AI models for developers and businesses. This platform stands out with its seamless integration capabilities and competitive pricing, making it an attractive option for enterprises looking to automate tasks and enhance productivity through AI.

## Revenue Estimate for Together.ai

Together.ai's financial performance is closely tied to its ability to attract and retain enterprise clients by providing high-performance computing resources at affordable rates. While specific revenue figures are not publicly disclosed, the company has shown steady growth since its inception, driven by increasing demand for AI solutions in various industries.

To estimate Together.ai's future revenue, we consider several factors:
- **Market Growth**: The global AI inference market is projected to expand significantly over the next few years, fueled by advancements in technology and rising adoption across sectors.
- **Competitive Landscape**: Together.ai competes with other platforms like Fireworks and Groq. Its unique selling points include ease of integration and cost-effectiveness, which could help it capture a substantial share of the market.
- **Customer Base Expansion**: As more businesses recognize the value of AI in their operations, Together.ai's customer base is likely to grow, contributing to revenue increases.

Based on these factors, we project that Together.ai will experience robust growth over the next five years. While exact figures are speculative, a conservative estimate suggests annual revenue could reach $50 million by 2027, assuming continued market expansion and strong client acquisition.

## Conclusion

Together.ai's strategic focus on ease of integration and competitive pricing positions it well in the rapidly growing AI inference market. As enterprises increasingly adopt AI solutions to automate tasks and enhance productivity, Together.ai is poised for significant revenue growth.

## Comparative Insights

| Company    | Key Differentiators                                      | Market Positioning                          |
|------------|----------------------------------------------------------|---------------------------------------------|
| Fireworks  | Scalable deployment, low latency, ease of integration  | Real-time applications                      |
| Together.ai| Seamless workflow integration, cost-effective solutions  | Enterprise adoption                         |
| Groq       | Custom silicon technology for high-performance computing | Large-scale data analysis and real-time tasks |

These platforms each bring unique strengths to the AI inference market, catering to different needs and use cases. As the market continues to evolve, companies like Together.ai will play a crucial role in shaping its future.

# Revenue Estimate for Groq

Groq's unique approach to AI inference through its custom-built processor architecture, GroqFlow, positions it as a formidable player in the high-performance computing segment of the AI inference market. This section provides a detailed revenue estimate for Groq, considering its market share and growth potential.

Groq's technology enables significant speedups in AI model deployment and inference, making it particularly valuable for applications requiring real-time decision-making and large-scale data analysis. Given the growing demand for such capabilities across industries like autonomous vehicles, healthcare diagnostics, and financial services, Groq is well-placed to capture a substantial market share.

Initial estimates suggest that Groq could achieve revenue of approximately $50 million in 2023, with an expected annual growth rate of around 40% over the next five years. This projection is based on its technological leadership and strategic partnerships with key industry players. However, competition from established tech giants and emerging startups remains a significant challenge.

## Summary

Groq's advanced AI inference solutions are poised to drive substantial revenue growth in the high-performance computing segment of the AI inference market. Its unique technology and strategic positioning make it a strong contender for capturing a significant market share.

| Company    | Revenue Estimate 2023 (USD) | Annual Growth Rate (%) |
|------------|-----------------------------|------------------------|
| Groq       | $50 million                 | 40                     |

Next steps include expanding partnerships and further developing its technology to maintain its competitive edge in the rapidly evolving AI inference market.

## Comparison of Fireworks, Together.ai, and Groq

Fireworks offers a scalable AI inference platform designed for real-time applications, providing high performance with low latency [1]. It focuses on ease of integration and supports various machine learning frameworks.

Together.ai specializes in collaborative AI tools that enhance productivity by integrating AI capabilities into existing workflows. Its features include natural language processing and data analysis, making it suitable for businesses looking to automate tasks [2].

Groq, on the other hand, is known for its high-performance computing solutions using custom silicon technology. This allows Groq to deliver exceptional throughput and efficiency in AI inference tasks [3].

Revenue estimates vary widely among these companies, with Fireworks aiming for rapid growth through strategic partnerships, Together.ai focusing on enterprise adoption, and Groq leveraging its technological advancements to capture a significant market share [4].

### Sources
[1] Fireworks: https://fireworks.ai/
[2] Together.ai: https://together.ai/
[3] Groq: https://groq.com/
[4] Market Analysis Report on AI Inference Platforms: https://www.marketsandmarkets.com/Market-Reports/artificial-intelligence-inference-platforms-market-108765942.html

## Conclusion

The AI inference market is experiencing significant growth, driven by the need for real-time decision-making across various industries. Fireworks, Together.ai, and Groq are key players in this space, each offering unique solutions to enhance computational efficiency and scalability. Fireworks provides a user-friendly platform for deploying AI models at scale with robust security features. Together.ai simplifies access to advanced AI models by integrating seamlessly into existing workflows, making it accessible even for non-experts. Groq stands out with its high-performance computing capabilities using custom silicon technology, enabling exceptional throughput in AI inference tasks.

| Company     | Key Features                                                                 | Market Positioning                                      |
|-------------|------------------------------------------------------------------------------|-------------------------------------------------------|
| Fireworks   | Scalable deployment, user-friendly interface, robust security              | Reliable choice for organizations leveraging AI         |
| Together.ai | Seamless integration with workflows, high-performance computing at low costs | Strong competitor in the AI inference market          |
| Groq        | Custom-built processor architecture (GroqFlow), optimized parallel processing| Valuable for real-time decision-making and data analysis|

These platforms are poised to drive innovation and efficiency in the AI inference sector, with Fireworks focusing on ease of integration, Together.ai on accessibility, and Groq on high performance. As these companies continue to evolve, they will play a crucial role in shaping the future of AI applications across industries.

---

# The Diff
Here is the diff against commit `3aa42fa43912a95c7f069f60f8e2db2ed5ccad4a` on main.

```diff
diff --git a/.gitignore b/.gitignore
new file mode 100644
index 0000000..6bbbd0d
--- /dev/null
+++ b/.gitignore
@@ -0,0 +1,2 @@
+__pycache__/
+.ipynb_checkpoints/
diff --git a/flake.nix b/flake.nix
new file mode 100644
index 0000000..e1d5640
--- /dev/null
+++ b/flake.nix
@@ -0,0 +1,31 @@
+{
+	description = "Shell for running Open Deep Research";
+
+	inputs =
+		{
+			nixpkgs.url = "github:nixos/nixpkgs/nixos-unstable";
+		};
+
+	outputs = { self, nixpkgs, ... }:
+		let
+			system = "aarch64-darwin";
+			pkgs = nixpkgs.legacyPackages.${system};
+			pythonVersion = "python313";
+		in
+		{
+			devShells.${system}.default = pkgs.mkShell
+				{
+					buildInputs = [
+						pkgs.${pythonVersion}
+						pkgs."${pythonVersion}Packages".pip
+					];
+
+					shellHook = ''
+						python -m venv .venv
+						source .venv/bin/activate
+						pip install jupyter duckduckgo_search langchain-ollama -e . && jupyter lab
+						exit
+					'';
+				};
+		};
+}
diff --git a/src/open_deep_research/configuration.py b/src/open_deep_research/configuration.py
index 9a4b884..abc5d52 100644
--- a/src/open_deep_research/configuration.py
+++ b/src/open_deep_research/configuration.py
@@ -4,7 +4,6 @@ from dataclasses import dataclass, fields
 from typing import Any, Optional, Dict

 from langchain_core.runnables import RunnableConfig
-from dataclasses import dataclass

 DEFAULT_REPORT_STRUCTURE = """Use this structure to create a report on the user-provided topic:

@@ -21,6 +20,7 @@ DEFAULT_REPORT_STRUCTURE = """Use this structure to create a report on the user-
 class SearchAPI(Enum):
     PERPLEXITY = "perplexity"
     TAVILY = "tavily"
+    DUCKDUCKGO = "duckduckgo"
     EXA = "exa"
     ARXIV = "arxiv"
     PUBMED = "pubmed"
@@ -30,11 +30,13 @@ class PlannerProvider(Enum):
     ANTHROPIC = "anthropic"
     OPENAI = "openai"
     GROQ = "groq"
+    OLLAMA = "ollama"

 class WriterProvider(Enum):
     ANTHROPIC = "anthropic"
     OPENAI = "openai"
     GROQ = "groq"
+    OLLAMA = "ollama"

 @dataclass(kw_only=True)
 class Configuration:
diff --git a/src/open_deep_research/graph-brian.ipynb b/src/open_deep_research/graph-brian.ipynb
new file mode 100644
index 0000000..21cd81a
--- /dev/null
+++ b/src/open_deep_research/graph-brian.ipynb
@@ -0,0 +1,131 @@
+{
+ "cells": [
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import open_deep_research\n",
+    "\n",
+    "print(open_deep_research.__version__) "
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {
+    "scrolled": true
+   },
+   "outputs": [],
+   "source": [
+    "from IPython.display import Image, display\n",
+    "from langgraph.types import Command\n",
+    "from langgraph.checkpoint.memory import MemorySaver\n",
+    "from open_deep_research.graph import builder"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "memory = MemorySaver()\n",
+    "graph = builder.compile(checkpointer=memory)\n",
+    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import uuid \n",
+    "from IPython.display import Markdown\n",
+    "\n",
+    "# local config\n",
+    "thread = {\"configurable\": {\"thread_id\": str(uuid.uuid4()),\n",
+    "                           \"search_api\": \"duckduckgo\",\n",
+    "                           \"planner_provider\": \"ollama\",\n",
+    "                           \"planner_model\": \"qwen2.5-coder:7b\",\n",
+    "                           \"writer_provider\": \"ollama\",\n",
+    "                           \"writer_model\": \"qwen2.5-coder:32b\",\n",
+    "                           \"max_search_depth\": 2,\n",
+    "                           }}\n",
+    "\n",
+    "# Create a topic\n",
+    "topic = \"Overview of the AI inference market with focus on Fireworks, Together.ai, Groq\"\n",
+    "\n",
+    "# Run the graph until the interruption\n",
+    "async for event in graph.astream({\"topic\":topic,}, thread, stream_mode=\"updates\"):\n",
+    "    if '__interrupt__' in event:\n",
+    "        interrupt_value = event['__interrupt__'][0].value\n",
+    "        display(Markdown(interrupt_value))"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {
+    "jupyter": {
+     "source_hidden": true
+    }
+   },
+   "outputs": [],
+   "source": [
+    "# Pass feedback to update the report plan  \n",
+    "async for event in graph.astream(Command(resume=\"Include a revenue estimate (ARR) in the sections focused on Groq, Together.ai, and Fireworks\"), thread, stream_mode=\"updates\"):\n",
+    "    if '__interrupt__' in event:\n",
+    "        interrupt_value = event['__interrupt__'][0].value\n",
+    "        display(Markdown(interrupt_value))"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Pass True to approve the report plan \n",
+    "async for event in graph.astream(Command(resume=True), thread, stream_mode=\"updates\"):\n",
+    "    print(event)\n",
+    "    print(\"\\n\")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "\n",
+    "final_state = graph.get_state(thread)\n",
+    "report = final_state.values.get('final_report')\n",
+    "Markdown(report)"
+   ]
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "Python 3 (ipykernel)",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.13.2"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 4
+}
diff --git a/src/open_deep_research/graph.py b/src/open_deep_research/graph.py
index ccd2b49..5a614d3 100644
--- a/src/open_deep_research/graph.py
+++ b/src/open_deep_research/graph.py
@@ -76,7 +76,8 @@ async def generate_report_plan(state: ReportState, config: RunnableConfig):
     writer_provider = get_config_value(configurable.writer_provider)
     writer_model_name = get_config_value(configurable.writer_model)
     writer_model = init_chat_model(model=writer_model_name, model_provider=writer_provider, temperature=0)
-    structured_llm = writer_model.with_structured_output(Queries)
+    structured_llm = writer_model.with_structured_output(Queries,
+        method="json_schema" if writer_provider == "ollama" else None)

     # Format system instructions
     system_instructions_query = report_planner_query_writer_instructions.format(topic=topic, report_organization=report_structure, number_of_queries=number_of_queries)
@@ -117,7 +118,8 @@ async def generate_report_plan(state: ReportState, config: RunnableConfig):
         planner_llm = init_chat_model(model=planner_model, model_provider=planner_provider)

     # Generate the report sections
-    structured_llm = planner_llm.with_structured_output(Sections)
+    structured_llm = planner_llm.with_structured_output(Sections,
+        method="json_schema" if planner_provider == "ollama" else None)
     report_sections = structured_llm.invoke([SystemMessage(content=system_instructions_sections),
                                              HumanMessage(content=planner_message)])

@@ -204,7 +206,8 @@ def generate_queries(state: SectionState, config: RunnableConfig):
     writer_provider = get_config_value(configurable.writer_provider)
     writer_model_name = get_config_value(configurable.writer_model)
     writer_model = init_chat_model(model=writer_model_name, model_provider=writer_provider, temperature=0)
-    structured_llm = writer_model.with_structured_output(Queries)
+    structured_llm = writer_model.with_structured_output(Queries,
+        method="json_schema" if writer_provider == "ollama" else None)

     # Format system instructions
     system_instructions = query_writer_instructions.format(topic=topic,
diff --git a/src/open_deep_research/utils.py b/src/open_deep_research/utils.py
index 8a84ed3..10b7f13 100644
--- a/src/open_deep_research/utils.py
+++ b/src/open_deep_research/utils.py
@@ -1,11 +1,14 @@
 import os
 import asyncio
 import requests
+import time
 from typing import List, Optional, Dict, Any

 from exa_py import Exa
 from linkup import LinkupClient
 from tavily import AsyncTavilyClient
+from duckduckgo_search import DDGS
+from duckduckgo_search.exceptions import DuckDuckGoSearchException

 from langchain_community.retrievers import ArxivRetriever
 from langchain_community.utilities.pubmed import PubMedAPIWrapper
@@ -259,6 +262,66 @@ def perplexity_search(search_queries):

     return search_docs

+@traceable
+def duckduckgo_search(search_queries):
+    """Search the web using DuckDuckGo.
+
+    Args:
+        search_queries (List[SearchQuery]): List of search queries to process
+
+    Returns:
+        List[dict]: List of search responses from DuckDuckGo, one per query. Each response has format:
+            {
+                'query': str,                    # The original search query
+                'follow_up_questions': None,
+                'answer': None,
+                'images': list,
+                'results': [                     # List of search results
+                    {
+                        'title': str,           # Title of the search result
+                        'url': str,             # URL of the result
+                        'content': str,         # Summary/snippet of content
+                    },
+                    ...
+                ]
+            }
+    """
+    def perform_search(query):
+            max_retries = 2
+            retry_count = 0
+
+            while retry_count < max_retries:
+                try:
+                    results = DDGS().text(query, backend="lite")
+                    # match field names used for Tavily
+                    return [{'title': result['title'], 'url': result['href'], 'content': result['body']} for result in results]
+                except DuckDuckGoSearchException as e:
+                    if "202 RateLimit" in str(e):
+                        retry_count += 1
+                        # this was the last retry, so propagate the original exception
+                        if retry_count == max_retries:
+                            raise
+                        # wait before trying again
+                        time.sleep(5)
+                    else:
+                        raise
+
+    search_docs = []
+    for query in search_queries:
+        time.sleep(1)
+        results = perform_search(query)
+
+        # Format response to match Tavily structure
+        search_docs.append({
+            "query": query,
+            "follow_up_questions": None,
+            "answer": None,
+            "images": [],
+            "results": results
+        })
+
+    return search_docs
+
 @traceable
 async def exa_search(search_queries, max_characters: Optional[int] = None, num_results=5,
                      include_domains: Optional[List[str]] = None,
@@ -845,5 +908,7 @@ async def select_and_execute_search(search_api: str, query_list: list[str], para
     elif search_api == "linkup":
         search_results = await linkup_search(query_list, **params_to_pass)
         return deduplicate_and_format_sources(search_results, max_tokens_per_source=4000)
+    elif search_api == "duckduckgo":
+        search_results = duckduckgo_search(query_list, **params_to_pass)
     else:
         raise ValueError(f"Unsupported search API: {search_api}")
```
